{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 01 — Data Cleaning & Coverage Audit (AIME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**What**: Cleaned AIME outputs; normalized key fields into a consistent view (year/variant/problem); and applied a strict quality gate (avg_neg_logprob > 0, total_neg_logprob > 0, non-empty token_neg_logprobs).\n",
        "\n",
        "**Why**: Ensure a trustworthy, uniform dataset before hardness modeling.\n",
        "\n",
        "**Results**: For every problem that exists in the source dataset, we now have ≥ 8 quality-passing generations per (year, variant, problem) cell (cap=8). Reasoning length (token count) emerges as a stable, informative signal; correct solutions tend to be shorter/more focused than incorrect ones. Logprobs are used for QC/diagnostics, not as modeling features.\n",
        "\n",
        "**Implication**: Clean, uniformly covered data + a minimal logprob gate ⇒ reliable comparisons and a simple, strong baseline feature (reasoning length) for the first hardness models.\n",
        "\n",
        "**Next**: Add a second small model, optionally increase generations, and build first baselines with reasoning length plus a few word-based cues."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0497afca",
      "metadata": {},
      "source": [
        "This week I cleaned the AIME generations and enforced a strict quality gate (non-empty token logprobs with positive averages/totals). After capping at 8 gens per year–problem cell, I rebuilt the plots directly from the filtered CSVs. The main signal that’s shaping up is response length (in tokens): correct solutions tend to be shorter/more focused than incorrect ones, which is promising for simple heuristic baselines and for a classifier feature set. Logprobs were useful for QC and filtering, but the length distribution is the more actionable metric here. I also verified coverage (AIME I pre-2000, I+II from 2000 on) and produced two rerun lists: (a) filesystem-based completeness, and (b) a dataset-aware list against di-zhang-fdu/AIME_1983_2024. Net-net: the pipeline is reproducible, coverage is where we expect it, and we have a reliable length feature to carry forward."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21582031",
      "metadata": {},
      "source": [
        "## 1) Load weekly report & sanity checks (counts, coverage, exclusions) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "\n",
              "### Report: `../results/week01_qstrict_cap8`\n",
              "- Selected records: **7,744**\n",
              "- Correct: **2,804**\n",
              "- Incorrect: **4,818**\n",
              "- Unknown: **122**\n",
              "- Accuracy on selected: **36.79%**\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "✅ Every (year, problem) cell has at least 8 selected generations."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "#### Exclusion reasons (counts)"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>reason</th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>failed_quality_gate</td>\n",
              "      <td>27952</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>hit_cell_cap</td>\n",
              "      <td>4864</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                reason  count\n",
              "0  failed_quality_gate  27952\n",
              "1         hit_cell_cap   4864"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# === 1) Load strict report & quick sanity checks (cap = 8 per (year, problem)) ===\n",
        "# Works even if pandas/matplotlib aren't installed in this Jupyter env.\n",
        "\n",
        "import os, csv, math\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "REPORT_DIR = \"../results/week01_qstrict_cap8\"\n",
        "\n",
        "# --- Optional deps (nice to have, but not required) ---\n",
        "try:\n",
        "    import pandas as pd\n",
        "except Exception:\n",
        "    pd = None\n",
        "\n",
        "try:\n",
        "    import matplotlib.pyplot as plt\n",
        "except Exception:\n",
        "    plt = None\n",
        "\n",
        "from IPython.display import display, Image, Markdown\n",
        "\n",
        "def load_csv(path):\n",
        "    if not os.path.isfile(path):\n",
        "        print(f\"[WARN] Missing CSV: {path}\")\n",
        "        return None\n",
        "    if pd is not None:\n",
        "        try:\n",
        "            return pd.read_csv(path)\n",
        "        except Exception:\n",
        "            pass\n",
        "    # csv.DictReader fallback\n",
        "    with open(path, newline=\"\", encoding=\"utf-8\") as f:\n",
        "        return list(csv.DictReader(f))\n",
        "\n",
        "# ---- Load data ----\n",
        "per_record_path = os.path.join(REPORT_DIR, \"per_record_stats.csv\")\n",
        "by_corr_path    = os.path.join(REPORT_DIR, \"by_correctness_summary.csv\")\n",
        "coverage_path   = os.path.join(REPORT_DIR, \"coverage_by_cell.csv\")\n",
        "excl_path       = os.path.join(REPORT_DIR, \"excluded_by_reason.csv\")\n",
        "\n",
        "per_record = load_csv(per_record_path)\n",
        "by_corr    = load_csv(by_corr_path)\n",
        "coverage   = load_csv(coverage_path)\n",
        "excluded   = load_csv(excl_path)\n",
        "\n",
        "# ---- Quick summary ----\n",
        "def as_rows(obj):\n",
        "    # normalize to list-of-dicts for simple fallbacks\n",
        "    if obj is None: return []\n",
        "    if pd is not None and hasattr(obj, \"to_dict\"):\n",
        "        return obj.to_dict(orient=\"records\")\n",
        "    return obj\n",
        "\n",
        "pr_rows = as_rows(per_record)\n",
        "\n",
        "total_selected = len(pr_rows)\n",
        "num_correct = sum(1 for r in pr_rows if str(r.get(\"correct\")).lower() == \"true\")\n",
        "num_incorrect = sum(1 for r in pr_rows if str(r.get(\"correct\")).lower() == \"false\")\n",
        "num_unknown = total_selected - num_correct - num_incorrect\n",
        "acc = (num_correct / max(1, (num_correct + num_incorrect))) * 100.0\n",
        "\n",
        "display(Markdown(f\"\"\"\n",
        "### Report: `{REPORT_DIR}`\n",
        "- Selected records: **{total_selected:,}**\n",
        "- Correct: **{num_correct:,}**\n",
        "- Incorrect: **{num_incorrect:,}**\n",
        "- Unknown: **{num_unknown:,}**\n",
        "- Accuracy on selected: **{acc:.2f}%**\n",
        "\"\"\"))\n",
        "\n",
        "# ---- Show already-rendered PNGs if matplotlib is missing in this notebook ----\n",
        "pngs = [\n",
        "    \"resp_len_boxplot.png\",\n",
        "    \"tok_len_boxplot.png\",\n",
        "    \"mean_nlp_hist_correct.png\",\n",
        "    \"mean_nlp_hist_incorrect.png\",\n",
        "]\n",
        "pngs = [p for p in pngs if os.path.isfile(os.path.join(REPORT_DIR, p))]\n",
        "\n",
        "if not pngs and plt is None:\n",
        "    print(\"[INFO] No PNGs found and matplotlib not available in this environment.\")\n",
        "\n",
        "if pngs and plt is None:\n",
        "    display(Markdown(\"#### Rendered figures from the container\"))\n",
        "    for p in pngs:\n",
        "        display(Image(filename=os.path.join(REPORT_DIR, p)))\n",
        "\n",
        "# ---- If matplotlib is available here, recreate a simple plot from CSV ----\n",
        "if plt is not None and pr_rows:\n",
        "    # Convert types\n",
        "    def to_float(x):\n",
        "        try:\n",
        "            if isinstance(x, bool): return math.nan\n",
        "            return float(x)\n",
        "        except Exception:\n",
        "            return math.nan\n",
        "\n",
        "    tok_len = [to_float(r.get(\"tok_len\")) for r in pr_rows]\n",
        "    is_corr = [str(r.get(\"correct\")).lower() == \"true\" for r in pr_rows]\n",
        "    is_inc  = [str(r.get(\"correct\")).lower() == \"false\" for r in pr_rows]\n",
        "\n",
        "    tok_corr = [t for t, c in zip(tok_len, is_corr) if not math.isnan(t) and c]\n",
        "    tok_inc  = [t for t, c in zip(tok_len, is_inc)  if not math.isnan(t) and c]\n",
        "\n",
        "    if tok_corr or tok_inc:\n",
        "        plt.figure()\n",
        "        data, labels = [], []\n",
        "        if tok_corr: data.append(tok_corr); labels.append(\"Correct\")\n",
        "        if tok_inc:  data.append(tok_inc);  labels.append(\"Incorrect\")\n",
        "        # deprecation fix: labels -> tick_labels\n",
        "        plt.boxplot(data, tick_labels=labels, showfliers=False)\n",
        "        plt.ylabel(\"Response length (tokens)\")\n",
        "        plt.title(\"Token Length by Correctness (quality-strict, cap=8)\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# ---- Coverage check: which (year, problem) cells are under the cap of 8? ----\n",
        "cov_rows = as_rows(coverage)\n",
        "if cov_rows:\n",
        "    underfilled = []\n",
        "    for r in cov_rows:\n",
        "        try:\n",
        "            y = int(r.get(\"year\"))\n",
        "            p = int(r.get(\"problem\"))\n",
        "            sel = int(r.get(\"selected\"))\n",
        "            if sel < 8:\n",
        "                underfilled.append((y, p, sel))\n",
        "        except Exception:\n",
        "            pass\n",
        "    underfilled.sort()\n",
        "    if underfilled:\n",
        "        display(Markdown(\"#### Cells with fewer than 8 selected generations\"))\n",
        "        head = \"\\n\".join(f\"- {y} / problem {p}: selected {sel}\" for (y,p,sel) in underfilled[:25])\n",
        "        display(Markdown(head))\n",
        "    else:\n",
        "        display(Markdown(\"✅ Every (year, problem) cell has at least 8 selected generations.\"))\n",
        "\n",
        "# ---- Exclusion reasons (why records were dropped) ----\n",
        "ex_rows = as_rows(excluded)\n",
        "if ex_rows:\n",
        "    display(Markdown(\"#### Exclusion reasons (counts)\"))\n",
        "    if pd is not None:\n",
        "        display(pd.DataFrame(ex_rows))\n",
        "    else:\n",
        "        for r in ex_rows:\n",
        "            print(f\"{r.get('reason')}: {r.get('count')}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Figures will be saved under `figures/week01`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9fac63b4",
      "metadata": {},
      "source": [
        "## 2) Generate figures (token length & mean −log p) → ../figures/week01"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "38bc9b89",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded: \n",
            "  per_record: 7744 rows \n",
            "  by_correctness_summary: 4 rows \n",
            "  coverage_by_cell: 596 rows\n",
            "Saved: ../figures/week01/01_tok_len_boxplot.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_3604804/2935997889.py:50: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n",
            "  plt.boxplot(data, labels=labels, showfliers=False)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: ../figures/week01/02_mean_nlp_hist_correct.png\n",
            "Saved: ../figures/week01/03_mean_nlp_hist_incorrect.png\n",
            "Done.\n"
          ]
        }
      ],
      "source": [
        "# ==== Params =========================================================\n",
        "REPORT_DIR = \"../reports/week01_qstrict_cap8\"   # folder produced by nb.py\n",
        "FIG_TAG    = \"week01\"                        # subfolder for saved figures\n",
        "# ====================================================================\n",
        "\n",
        "import os, sys, importlib, subprocess\n",
        "\n",
        "# Auto-install if missing (keeps the cell self-contained)\n",
        "for pkg in [\"matplotlib\", \"pandas\"]:\n",
        "    try:\n",
        "        importlib.import_module(pkg)\n",
        "    except ImportError:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg])\n",
        "\n",
        "# Safe headless plotting\n",
        "import matplotlib\n",
        "if \"DISPLAY\" not in os.environ:\n",
        "    matplotlib.use(\"Agg\")\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "FIG_DIR = os.path.join(\"..\",\"figures\", FIG_TAG)\n",
        "os.makedirs(FIG_DIR, exist_ok=True)\n",
        "\n",
        "def _read_csv(path):\n",
        "    if os.path.exists(path):\n",
        "        return pd.read_csv(path)\n",
        "    print(f\"[WARN] Missing CSV: {path}\")\n",
        "    return None\n",
        "\n",
        "per_record = _read_csv(os.path.join(REPORT_DIR, \"per_record_stats.csv\"))\n",
        "by_corr    = _read_csv(os.path.join(REPORT_DIR, \"by_correctness_summary.csv\"))\n",
        "coverage   = _read_csv(os.path.join(REPORT_DIR, \"coverage_by_cell.csv\"))\n",
        "\n",
        "print(\"Loaded:\",\n",
        "      f\"\\n  per_record: {0 if per_record is None else len(per_record)} rows\",\n",
        "      f\"\\n  by_correctness_summary: {0 if by_corr is None else len(by_corr)} rows\",\n",
        "      f\"\\n  coverage_by_cell: {0 if coverage is None else len(coverage)} rows\")\n",
        "\n",
        "# --- Fig 1: Token length boxplot (Correct vs Incorrect) ---\n",
        "if per_record is not None and {\"tok_len\",\"correct\"}.issubset(per_record.columns):\n",
        "    df = per_record.dropna(subset=[\"tok_len\",\"correct\"])\n",
        "    data, labels = [], []\n",
        "    if not df[df[\"correct\"] == True].empty:\n",
        "        data.append(df[df[\"correct\"] == True][\"tok_len\"].tolist()); labels.append(\"Correct\")\n",
        "    if not df[df[\"correct\"] == False].empty:\n",
        "        data.append(df[df[\"correct\"] == False][\"tok_len\"].tolist()); labels.append(\"Incorrect\")\n",
        "    if data:\n",
        "        plt.figure()\n",
        "        plt.boxplot(data, labels=labels, showfliers=False)\n",
        "        plt.ylabel(\"Response length (tokens)\")\n",
        "        plt.title(\"Token Length by Correctness\")\n",
        "        plt.tight_layout()\n",
        "        out1 = os.path.join(FIG_DIR, \"01_tok_len_boxplot.png\")\n",
        "        plt.savefig(out1, dpi=150); plt.show()\n",
        "        print(\"Saved:\", out1)\n",
        "\n",
        "# --- Fig 2a/b: Mean negative logprob histograms ---\n",
        "if per_record is not None and {\"mean_negative_logprob\",\"correct\"}.issubset(per_record.columns):\n",
        "    d = per_record.dropna(subset=[\"mean_negative_logprob\",\"correct\"])\n",
        "    dc  = d[d[\"correct\"] == True]\n",
        "    dic = d[d[\"correct\"] == False]\n",
        "    if not dc.empty:\n",
        "        plt.figure(); plt.hist(dc[\"mean_negative_logprob\"].tolist(), bins=40)\n",
        "        plt.xlabel(\"Mean negative logprob\"); plt.ylabel(\"Count\"); plt.title(\"Mean -log p — Correct\")\n",
        "        plt.tight_layout()\n",
        "        out2a = os.path.join(FIG_DIR, \"02_mean_nlp_hist_correct.png\")\n",
        "        plt.savefig(out2a, dpi=150); plt.show(); print(\"Saved:\", out2a)\n",
        "    if not dic.empty:\n",
        "        plt.figure(); plt.hist(dic[\"mean_negative_logprob\"].tolist(), bins=40)\n",
        "        plt.xlabel(\"Mean negative logprob\"); plt.ylabel(\"Count\"); plt.title(\"Mean -log p — Incorrect\")\n",
        "        plt.tight_layout()\n",
        "        out2b = os.path.join(FIG_DIR, \"03_mean_nlp_hist_incorrect.png\")\n",
        "        plt.savefig(out2b, dpi=150); plt.show(); print(\"Saved:\", out2b)\n",
        "\n",
        "print(\"Done.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7d51970",
      "metadata": {},
      "source": [
        "## 3. Coverage audit w/ AIME variant (I/II): underfilled cells + heatmaps"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81ecc2db",
      "metadata": {},
      "source": [
        "Point: sanity-check that coverage is complete per AIME variant (I vs II). It verifies that, after your quality filter, you still have ≥ 8 generations for every (year, variant, problem), and it visualizes that coverage.\n",
        "\n",
        "What it catches: mis-labeled paths (e.g., an “II” folder missed), holes that were hidden when variants were merged, or cells that slipped below 8 after filtering."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "d779ab37",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Underfilled (<8) cells: 0\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>variant</th>\n",
              "      <th>problem</th>\n",
              "      <th>selected</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [year, variant, problem, selected]\n",
              "Index: []"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import os, re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Pick one of the reports you generated inside the container:\n",
        "# \"reports/scan_results_only_qstrict_cap8\"  (1983–2009)\n",
        "# \"reports/scan_exp_only_qstrict_cap8\"      (2010–2024)\n",
        "#REPORT_DIR = \"../reports/scan_exp_only_qstrict_cap8\"\n",
        "REPORT_DIR = \"../reports/scan_results_only_qstrict_cap8\"\n",
        "\n",
        "pr_path = os.path.join(REPORT_DIR, \"per_record_stats.csv\")\n",
        "pr = pd.read_csv(pr_path)\n",
        "\n",
        "def parse_variant_from_path(path, year):\n",
        "    p = str(path).upper()\n",
        "    if \"/II/\" in p or p.endswith(\"/II\"): return \"II\"\n",
        "    if \"/I/\"  in p or p.endswith(\"/I\"):  return \"I\"\n",
        "    # Pre-2000 AIME only has I\n",
        "    try:\n",
        "        if int(year) <= 1999: return \"I\"\n",
        "    except Exception:\n",
        "        pass\n",
        "    return None  # unknown; will show as underfilled\n",
        "\n",
        "pr[\"variant\"] = [parse_variant_from_path(p, y) for p, y in zip(pr[\"path\"], pr[\"year\"])]\n",
        "\n",
        "# Coverage with variant\n",
        "cov = (pr.groupby([\"year\",\"variant\",\"problem\"])\n",
        "         .size()\n",
        "         .reset_index(name=\"selected\"))\n",
        "\n",
        "# Quick check: cells with < 8 selected\n",
        "under = cov[cov[\"selected\"] < 8].sort_values([\"year\",\"variant\",\"problem\"])\n",
        "print(\"Underfilled (<8) cells:\", len(under))\n",
        "display(under.head(30))\n",
        "\n",
        "# Heatmaps by variant\n",
        "for v in sorted(cov[\"variant\"].dropna().unique()):\n",
        "    sub = cov[cov[\"variant\"]==v]\n",
        "    piv = sub.pivot_table(index=\"year\", columns=\"problem\", values=\"selected\", fill_value=0).sort_index()\n",
        "    Z = np.clip(piv.to_numpy(), 0, 8)\n",
        "    plt.figure(figsize=(9,6))\n",
        "    im = plt.imshow(Z, aspect=\"auto\", origin=\"lower\", cmap=\"viridis\", vmin=0, vmax=8)\n",
        "    plt.colorbar(im, label=\"# selected (cap=8)\")\n",
        "    plt.yticks(range(len(piv.index)), piv.index.tolist())\n",
        "    plt.xticks(range(len(piv.columns)), piv.columns.tolist())\n",
        "    plt.xlabel(\"Problem\"); plt.ylabel(\"Year\")\n",
        "    plt.title(f\"Coverage (selected per year/problem) — AIME {v}\")\n",
        "    plt.tight_layout(); plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0141041d",
      "metadata": {},
      "source": [
        "## 4) Filesystem audit (AIME I/II) + quality filter + rerun checklist — outputs only\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c50ebe8",
      "metadata": {},
      "source": [
        "Scope: checks generated files vs. the expected 1–15 grid.\n",
        "\n",
        "Output: aime_rerun_todo_fs.csv."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "389b733d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "year  variants  problems_I  problems_II  expected  selected_rows(approx)\n",
            "1983  I        15          0           15       120\n",
            "1984  I        14          0           15       112\n",
            "1985  I        15          0           15       120\n",
            "1986  I        14          0           15       112\n",
            "1987  I        14          0           15       112\n",
            "1988  I        10          0           15       80\n",
            "1989  I        9           0           15       72\n",
            "1990  I        12          0           15       96\n",
            "1991  I        11          0           15       88\n",
            "1992  I        15          0           15       120\n",
            "1993  I        14          0           15       112\n",
            "1994  I        9           0           15       72\n",
            "1995  I        14          0           15       112\n",
            "1996  I        14          0           15       112\n",
            "1997  I        14          0           15       112\n",
            "1998  I        15          0           15       120\n",
            "1999  I        15          0           15       120\n",
            "2000  I+II     15          12          30       216\n",
            "2001  I+II     15          15          30       240\n",
            "2002  I+II     14          14          30       224\n",
            "2003  I+II     14          14          30       224\n",
            "2004  I+II     14          15          30       232\n",
            "2005  I+II     14          15          30       232\n",
            "2006  I+II     14          14          30       224\n",
            "2007  I+II     15          13          30       224\n",
            "2008  I+II     14          13          30       216\n",
            "2009  I+II     15          15          30       240\n",
            "2010  I+II     15          13          30       224\n",
            "2011  I+II     14          15          30       232\n",
            "2012  I+II     12          14          30       208\n",
            "2013  I+II     14          15          30       232\n",
            "2014  I+II     15          15          30       240\n",
            "2015  I+II     15          15          30       240\n",
            "2016  I+II     15          15          30       240\n",
            "2017  I+II     15          15          30       240\n",
            "2018  I+II     15          15          30       240\n",
            "2019  I+II     15          15          30       240\n",
            "2020  I+II     15          15          30       240\n",
            "2021  I+II     15          15          30       240\n",
            "2022  I+II     15          15          30       240\n",
            "2023  I+II     14          15          30       232\n",
            "2024  II       0           14          30       112\n",
            "\n",
            "=== Missing problems (no quality-passing gens) ===\n",
            "  1984 I: missing [15]\n",
            "  1986 I: missing [4]\n",
            "  1987 I: missing [13]\n",
            "  1988 I: missing [1, 4, 6, 11, 12]\n",
            "  1989 I: missing [3, 6, 10, 12, 14, 15]\n",
            "  1990 I: missing [4, 8, 12]\n",
            "  1991 I: missing [3, 6, 7, 15]\n",
            "  1993 I: missing [3]\n",
            "  1994 I: missing [3, 4, 5, 6, 7, 13]\n",
            "  1995 I: missing [7]\n",
            "  1996 I: missing [1]\n",
            "  1997 I: missing [13]\n",
            "  2000 II: missing [1, 7, 15]\n",
            "  2002 I: missing [7]\n",
            "  2002 II: missing [7]\n",
            "  2003 I: missing [1]\n",
            "  2003 II: missing [15]\n",
            "  2004 I: missing [8]\n",
            "  2005 I: missing [13]\n",
            "  2006 I: missing [11]\n",
            "  2006 II: missing [4]\n",
            "  2007 II: missing [3, 8]\n",
            "  2008 I: missing [15]\n",
            "  2008 II: missing [10, 15]\n",
            "  2010 II: missing [8, 11]\n",
            "  2011 I: missing [8]\n",
            "  2012 I: missing [7, 8, 15]\n",
            "  2012 II: missing [5]\n",
            "  2013 I: missing [2]\n",
            "  2023 I: missing [15]\n",
            "  2024 I: missing [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
            "  2024 II: missing [9]\n",
            "\n",
            "=== Underfilled cells (< 8 quality-passing gens) ===\n",
            "None\n",
            "\n",
            "Scanned files: 22904\n",
            "Saved rerun checklist -> ../reports/aime_rerun_todo_fs.csv\n"
          ]
        }
      ],
      "source": [
        "# Filesystem coverage audit (AIME I/II) with quality filter and rerun checklist\n",
        "import os, re, json, csv, math, collections\n",
        "\n",
        "# --- Config: adjust if your notebook sits elsewhere ---\n",
        "ROOTS = [\"../results\", \"../experiment_archive\"]   # both trees scanned recursively\n",
        "QUALITY_STRICT = True                              # set False to see raw presence (ignores logprob quality)\n",
        "CAP_PER_CELL = 8                                   # target gens per (year, variant, problem)\n",
        "\n",
        "YEAR_RE   = re.compile(r'^(19[8-9]\\d|20[0-2]\\d)$')  # 1980–2029 safe range\n",
        "VARIANTS  = {\"I\",\"II\"}\n",
        "\n",
        "def safe_json_load(path):\n",
        "    try:\n",
        "        with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "            return json.load(f)\n",
        "    except Exception:\n",
        "        # try JSONL\n",
        "        try:\n",
        "            with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "                first = f.readline()\n",
        "                return json.loads(first) if first.strip() else None\n",
        "        except Exception:\n",
        "            return None\n",
        "\n",
        "def quality_ok(rec):\n",
        "    if not QUALITY_STRICT:\n",
        "        return True\n",
        "    try:\n",
        "        avg   = rec.get(\"avg_neg_logprob\")\n",
        "        total = rec.get(\"total_neg_logprob\") or rec.get(\"sum_neg_logprob\") or rec.get(\"neg_logprob_total\")\n",
        "        toks  = rec.get(\"token_neg_logprobs\")\n",
        "        if not (isinstance(avg,(int,float)) and avg > 0): return False\n",
        "        if not (isinstance(total,(int,float)) and total > 0): return False\n",
        "        if not (isinstance(toks, list) and len(toks) > 0):    return False\n",
        "        return True\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "def parse_cell_from_path(path):\n",
        "    \"\"\"\n",
        "    Extract (year:int, variant:'I'|'II' or None, problem:int|None) from a path like:\n",
        "    .../aime/.../<year>/<variant>/<problem>/.../*.json\n",
        "    Robust to the deep experiment_archive timestamp prefix.\n",
        "    \"\"\"\n",
        "    parts = os.path.normpath(path).split(os.sep)\n",
        "    # walk parts; when we see a year, look ahead for variant & problem\n",
        "    for i, part in enumerate(parts):\n",
        "        if YEAR_RE.match(part):\n",
        "            year = int(part)\n",
        "            variant = None\n",
        "            problem = None\n",
        "            # variant right after year?\n",
        "            if i+1 < len(parts) and parts[i+1] in VARIANTS:\n",
        "                variant = parts[i+1]\n",
        "                # problem right after variant?\n",
        "                if i+2 < len(parts) and parts[i+2].isdigit():\n",
        "                    p = int(parts[i+2])\n",
        "                    if 1 <= p <= 15: problem = p\n",
        "            else:\n",
        "                # no explicit variant; for pre-2000 default to I if problem follows\n",
        "                if year <= 1999:\n",
        "                    if i+1 < len(parts) and parts[i+1].isdigit():\n",
        "                        p = int(parts[i+1])\n",
        "                        if 1 <= p <= 15:\n",
        "                            variant = \"I\"\n",
        "                            problem = p\n",
        "            if year and (variant or year <= 1999) and problem:\n",
        "                # if still no variant post-1999, leave None (we'll skip those)\n",
        "                if variant is None and year <= 1999:\n",
        "                    variant = \"I\"\n",
        "                return year, variant, problem\n",
        "    return None, None, None\n",
        "\n",
        "# Scan filesystem\n",
        "cells_total     = collections.defaultdict(int)      # raw file count per cell\n",
        "cells_quality   = collections.defaultdict(int)      # quality-passing file count per cell\n",
        "problems_by_yv  = collections.defaultdict(set)      # problems present (quality-passing) per (year,variant)\n",
        "variants_by_y   = collections.defaultdict(set)      # variants seen per year (quality-passing)\n",
        "\n",
        "scanned_files = 0\n",
        "for root in ROOTS:\n",
        "    if not os.path.isdir(root):\n",
        "        print(f\"[WARN] Missing root: {root}\")\n",
        "        continue\n",
        "    for dirpath, _, files in os.walk(root):\n",
        "        for fn in files:\n",
        "            if not fn.lower().endswith((\".json\",\".jsonl\",\".ndjson\")):\n",
        "                continue\n",
        "            fpath = os.path.join(dirpath, fn)\n",
        "            y, v, p = parse_cell_from_path(fpath)\n",
        "            if y is None or p is None:\n",
        "                continue\n",
        "            if y >= 2000 and v not in VARIANTS:\n",
        "                # post-1999 must explicitly have I or II in path\n",
        "                continue\n",
        "            if y <= 1999 and v is None:\n",
        "                v = \"I\"\n",
        "            scanned_files += 1\n",
        "            cells_total[(y,v,p)] += 1\n",
        "            rec = safe_json_load(fpath)\n",
        "            if rec is None:\n",
        "                continue\n",
        "            if quality_ok(rec):\n",
        "                cells_quality[(y,v,p)] += 1\n",
        "                problems_by_yv[(y,v)].add(p)\n",
        "                variants_by_y[y].add(v)\n",
        "\n",
        "def vset_string(vset):\n",
        "    return \"+\".join(sorted(vset)) if vset else \"I\"\n",
        "\n",
        "# Summary table (quality-passing presence)\n",
        "print(\"year  variants  problems_I  problems_II  expected  selected_rows(approx)\")\n",
        "for y in sorted({y for (y,_,_) in cells_total} | set(variants_by_y.keys())):\n",
        "    vset = variants_by_y.get(y, set())\n",
        "    probs_I  = len(problems_by_yv.get((y,\"I\"), set()))\n",
        "    probs_II = len(problems_by_yv.get((y,\"II\"), set()))\n",
        "    expected = 15 if y <= 1999 else 30\n",
        "    approx_selected = (probs_I + probs_II) * CAP_PER_CELL\n",
        "    print(f\"{y:<5} {vset_string(vset):<8} {probs_I:<11} {probs_II:<11} {expected:<8} {approx_selected}\")\n",
        "\n",
        "# Missing problems (no quality-passing gen found)\n",
        "missing = []\n",
        "for y in sorted({y for (y,_,_) in cells_total} | set(variants_by_y.keys())):\n",
        "    targets = [\"I\"] if y <= 1999 else [\"I\",\"II\"]\n",
        "    for v in targets:\n",
        "        seen = problems_by_yv.get((y,v), set())\n",
        "        expected_probs = set(range(1,16))\n",
        "        miss = sorted(expected_probs - seen)\n",
        "        if miss:\n",
        "            missing.append((y, v, miss))\n",
        "\n",
        "print(\"\\n=== Missing problems (no quality-passing gens) ===\")\n",
        "if missing:\n",
        "    for y, v, miss in missing[:120]:\n",
        "        print(f\"  {y} {v}: missing {miss}\")\n",
        "else:\n",
        "    print(\"None\")\n",
        "\n",
        "# Underfilled (< CAP_PER_CELL) among quality-passing cells\n",
        "underfilled = []\n",
        "for (y,v,p), n in cells_quality.items():\n",
        "    if n < CAP_PER_CELL:\n",
        "        underfilled.append((y,v,p,n))\n",
        "\n",
        "print(\"\\n=== Underfilled cells (< {} quality-passing gens) ===\".format(CAP_PER_CELL))\n",
        "if underfilled:\n",
        "    for y,v,p,n in sorted(underfilled)[:200]:\n",
        "        print(f\"  {y} {v} problem {p}: {n} gens\")\n",
        "else:\n",
        "    print(\"None\")\n",
        "\n",
        "# Save rerun checklist (missing -> 8, underfilled -> top-up)\n",
        "todo = []\n",
        "for y, v, miss in missing:\n",
        "    for p in miss:\n",
        "        todo.append({\"year\": y, \"variant\": v, \"problem\": p, \"needed\": CAP_PER_CELL})\n",
        "for y,v,p,n in underfilled:\n",
        "    todo.append({\"year\": y, \"variant\": v, \"problem\": p, \"needed\": CAP_PER_CELL - n})\n",
        "\n",
        "os.makedirs(\"../reports\", exist_ok=True)\n",
        "todo_path = \"../reports/aime_rerun_todo_fs.csv\"\n",
        "with open(todo_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "    w = csv.DictWriter(f, fieldnames=[\"year\",\"variant\",\"problem\",\"needed\"])\n",
        "    w.writeheader()\n",
        "    for r in sorted(todo, key=lambda r:(r[\"year\"], r[\"variant\"], r[\"problem\"])):\n",
        "        w.writerow(r)\n",
        "\n",
        "print(f\"\\nScanned files: {scanned_files}\")\n",
        "print(f\"Saved rerun checklist -> {todo_path}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b769eb9",
      "metadata": {},
      "source": [
        "## 5) AIME dataset cross-check (expected vs. generated) + rerun checklist — detects dataset omissions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6efc2335",
      "metadata": {},
      "source": [
        "Scope: uses di-zhang-fdu/AIME_1983_2024 to define expected problems, then compares to outputs with the quality filter.\n",
        "\n",
        " Output: aime_rerun_todo_hf.csv."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "7b7868b9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: datasets in /home/user1/.local/lib/python3.10/site-packages (4.0.0)\n",
            "Requirement already satisfied: xxhash in /home/user1/.local/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from datasets) (5.4.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /home/user1/.local/lib/python3.10/site-packages (from datasets) (21.0.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /home/user1/.local/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: packaging in /home/user1/.local/lib/python3.10/site-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /home/user1/.local/lib/python3.10/site-packages (from datasets) (2.2.6)\n",
            "Requirement already satisfied: fsspec[http]<=2025.3.0,>=2023.1.0 in /home/user1/.local/lib/python3.10/site-packages (from datasets) (2025.3.0)\n",
            "Requirement already satisfied: pandas in /home/user1/.local/lib/python3.10/site-packages (from datasets) (2.3.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /home/user1/.local/lib/python3.10/site-packages (from datasets) (2.32.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /home/user1/.local/lib/python3.10/site-packages (from datasets) (0.34.4)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/user1/.local/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /home/user1/.local/lib/python3.10/site-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: filelock in /home/user1/.local/lib/python3.10/site-packages (from datasets) (3.19.1)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/user1/.local/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/user1/.local/lib/python3.10/site-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/user1/.local/lib/python3.10/site-packages (from huggingface-hub>=0.24.0->datasets) (1.1.9)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /home/user1/.local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.32.2->datasets) (3.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests>=2.32.2->datasets) (1.26.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.32.2->datasets) (2020.6.20)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /home/user1/.local/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /home/user1/.local/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas->datasets) (2022.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/user1/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /home/user1/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/user1/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (5.0.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /home/user1/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/user1/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/user1/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /home/user1/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /home/user1/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/user1/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "year  variants  problems_I  problems_II  expected_total\n",
            "\n",
            "=== Missing (no quality-passing gens but present in HF dataset) ===\n",
            "None\n",
            "\n",
            "=== Underfilled (< 8 quality-passing gens) ===\n",
            "None\n",
            "\n",
            "Scanned files: 22904\n",
            "HF rows with unparsed y/v/p: 933\n",
            "Saved rerun checklist -> ../reports/aime_rerun_todo_hf.csv\n"
          ]
        }
      ],
      "source": [
        "# Audit coverage against HF dataset \"di-zhang-fdu/AIME_1983_2024\"\n",
        "# and local outputs (quality-strict; cap=8)\n",
        "\n",
        "import os, re, json, csv, collections, math\n",
        "\n",
        "# ---- Config ----\n",
        "ROOTS_OUTPUT = [\"../results\", \"../experiment_archive\"]  # your generated outputs\n",
        "CAP_PER_CELL = 8\n",
        "QUALITY_STRICT = True\n",
        "\n",
        "# ---- Helpers ----\n",
        "YEAR_RE = re.compile(r'^(19[8-9]\\d|20[0-2]\\d)$')  # 1980–2029\n",
        "VARIANTS = {\"I\", \"II\"}  # restrict to I/II only\n",
        "\n",
        "def safe_json_load(path):\n",
        "    try:\n",
        "        with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "            # try JSON object\n",
        "            return json.load(f)\n",
        "    except Exception:\n",
        "        # try first line JSONL\n",
        "        try:\n",
        "            with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "                line = f.readline()\n",
        "                return json.loads(line) if line.strip() else None\n",
        "        except Exception:\n",
        "            return None\n",
        "\n",
        "def parse_cell_from_path(path):\n",
        "    \"\"\"Extract (year:int, variant:'I'|'II', problem:int) from .../<year>/<I|II>/<1..15>/... (or pre-2000 /<year>/<1..15>/...)\"\"\"\n",
        "    parts = os.path.normpath(path).split(os.sep)\n",
        "    for i, part in enumerate(parts):\n",
        "        if YEAR_RE.match(part):\n",
        "            y = int(part)\n",
        "            v = None\n",
        "            p = None\n",
        "            if i+1 < len(parts) and parts[i+1] in VARIANTS:\n",
        "                v = parts[i+1]\n",
        "                if i+2 < len(parts) and parts[i+2].isdigit():\n",
        "                    pp = int(parts[i+2])\n",
        "                    if 1 <= pp <= 15: p = pp\n",
        "            else:\n",
        "                # pre-2000: only AIME I, problems live directly under year/\n",
        "                if y <= 1999 and i+1 < len(parts) and parts[i+1].isdigit():\n",
        "                    pp = int(parts[i+1])\n",
        "                    if 1 <= pp <= 15:\n",
        "                        v = \"I\"; p = pp\n",
        "            if y and v in VARIANTS and p:\n",
        "                return y, v, p\n",
        "    return None, None, None\n",
        "\n",
        "def quality_ok(rec):\n",
        "    if not QUALITY_STRICT:\n",
        "        return True\n",
        "    try:\n",
        "        avg   = rec.get(\"avg_neg_logprob\")\n",
        "        total = rec.get(\"total_neg_logprob\") or rec.get(\"sum_neg_logprob\") or rec.get(\"neg_logprob_total\")\n",
        "        toks  = rec.get(\"token_neg_logprobs\")\n",
        "        return (isinstance(avg,(int,float)) and avg > 0 and\n",
        "                isinstance(total,(int,float)) and total > 0 and\n",
        "                isinstance(toks, list) and len(toks) > 0)\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "# ---- 1) Build EXPECTED inventory from HF dataset ----\n",
        "try:\n",
        "    !pip install datasets\n",
        "    from datasets import load_dataset\n",
        "except Exception as e:\n",
        "    raise RuntimeError(\"You need `pip install datasets` in this environment.\") from e\n",
        "\n",
        "ds = load_dataset(\"di-zhang-fdu/AIME_1983_2024\")\n",
        "splits = [ds[k] for k in ds.keys()]  # train/validation/etc., whatever exists\n",
        "\n",
        "def infer_yvp(ex):\n",
        "    \"\"\"\n",
        "    Robustly infer (year, variant, problem) from a dataset example.\n",
        "    Tries common field names; falls back to parsing a title/source string.\n",
        "    \"\"\"\n",
        "    # common explicit fields\n",
        "    y = ex.get(\"year\")\n",
        "    v = ex.get(\"variant\") or ex.get(\"exam\") or ex.get(\"set\")\n",
        "    p = ex.get(\"problem_number\") or ex.get(\"problem_index\") or ex.get(\"question_number\") or ex.get(\"index\")\n",
        "\n",
        "    # normalize types\n",
        "    try: y = int(y) if y is not None and str(y).isdigit() else None\n",
        "    except: y = None\n",
        "    v = str(v).strip().upper() if isinstance(v, str) else None\n",
        "    try: p = int(p) if p is not None and str(p).isdigit() else None\n",
        "    except: p = None\n",
        "\n",
        "    # If still missing, parse a composite string if present\n",
        "    blob = None\n",
        "    for k in (\"source\", \"title\", \"name\", \"id\"):\n",
        "        if isinstance(ex.get(k), str) and ex[k]:\n",
        "            blob = ex[k]\n",
        "            break\n",
        "    if blob:\n",
        "        # e.g., \"AIME 2006 II Problem 7\"\n",
        "        m = re.search(r\"AIME\\s+(19[8-9]\\d|20[0-2]\\d)\\s+(I{1,3}|II)\\b.*?(?:Problem\\s+(\\d{1,2}))?\", blob, re.I)\n",
        "        if m:\n",
        "            y = y or int(m.group(1))\n",
        "            v = v or m.group(2).upper().replace(\"III\",\"II\")  # sanitize weird \"III\"\n",
        "            if not p and m.group(3): p = int(m.group(3))\n",
        "\n",
        "    # defaults: before 2000 only I\n",
        "    if y and y <= 1999 and not v:\n",
        "        v = \"I\"\n",
        "\n",
        "    # final guardrails\n",
        "    if y and (v in VARIANTS) and p and 1 <= p <= 15:\n",
        "        return y, v, p\n",
        "    return None, None, None\n",
        "\n",
        "expected = collections.defaultdict(set)  # (year, variant) -> {problems}\n",
        "bad_rows = 0\n",
        "for split in splits:\n",
        "    for ex in split:\n",
        "        y, v, p = infer_yvp(ex)\n",
        "        if y and v and p:\n",
        "            expected[(y, v)].add(p)\n",
        "        else:\n",
        "            bad_rows += 1\n",
        "\n",
        "# ---- 2) Count SEEN quality-passing gens from local outputs (no cap) ----\n",
        "cells_quality = collections.defaultdict(int)   # (y,v,p) -> count\n",
        "scanned_files = 0\n",
        "for root in ROOTS_OUTPUT:\n",
        "    if not os.path.isdir(root): continue\n",
        "    for dirpath, _, files in os.walk(root):\n",
        "        for fn in files:\n",
        "            if not fn.lower().endswith((\".json\",\".jsonl\",\".ndjson\")):\n",
        "                continue\n",
        "            fpath = os.path.join(dirpath, fn)\n",
        "            y, v, p = parse_cell_from_path(fpath)\n",
        "            if not (y and v and p): continue\n",
        "            rec = safe_json_load(fpath)\n",
        "            scanned_files += 1\n",
        "            if rec is None: continue\n",
        "            if quality_ok(rec):\n",
        "                cells_quality[(y, v, p)] += 1\n",
        "\n",
        "# ---- 3) Summaries ----\n",
        "def vstr(vs): return \"+\".join(sorted(vs)) if vs else \"I\"\n",
        "\n",
        "years = sorted({y for (y,_) in expected})\n",
        "print(\"year  variants  problems_I  problems_II  expected_total\")\n",
        "for y in years:\n",
        "    vset = {v for (yy,v) in expected if yy == y}\n",
        "    pI   = len(expected.get((y,\"I\"), set()))\n",
        "    pII  = len(expected.get((y,\"II\"), set()))\n",
        "    print(f\"{y:<5} {vstr(vset):<8} {pI:<11} {pII:<11} {pI + pII}\")\n",
        "\n",
        "# ---- 4) Missing/Underfilled relative to HF inventory ----\n",
        "missing = []\n",
        "underfilled = []\n",
        "for (y,v), exp_probs in sorted(expected.items()):\n",
        "    for p in sorted(exp_probs):\n",
        "        n = cells_quality.get((y,v,p), 0)\n",
        "        if n == 0:\n",
        "            missing.append((y, v, p))\n",
        "        elif n < CAP_PER_CELL:\n",
        "            underfilled.append((y, v, p, n))\n",
        "\n",
        "print(\"\\n=== Missing (no quality-passing gens but present in HF dataset) ===\")\n",
        "if missing:\n",
        "    for y,v,p in missing[:150]:\n",
        "        print(f\"  {y} {v} problem {p}\")\n",
        "else:\n",
        "    print(\"None\")\n",
        "\n",
        "print(\"\\n=== Underfilled (< {} quality-passing gens) ===\".format(CAP_PER_CELL))\n",
        "if underfilled:\n",
        "    for y,v,p,n in underfilled[:150]:\n",
        "        print(f\"  {y} {v} problem {p}: {n}\")\n",
        "else:\n",
        "    print(\"None\")\n",
        "\n",
        "# ---- 5) Rerun checklist (only what HF says should exist) ----\n",
        "os.makedirs(\"../reports\", exist_ok=True)\n",
        "todo_path = \"../reports/aime_rerun_todo_hf.csv\"\n",
        "with open(todo_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "    w = csv.DictWriter(f, fieldnames=[\"year\",\"variant\",\"problem\",\"needed\"])\n",
        "    w.writeheader()\n",
        "    for y,v,p in missing:\n",
        "        w.writerow({\"year\": y, \"variant\": v, \"problem\": p, \"needed\": CAP_PER_CELL})\n",
        "    for y,v,p,n in underfilled:\n",
        "        w.writerow({\"year\": y, \"variant\": v, \"problem\": p, \"needed\": CAP_PER_CELL - n})\n",
        "\n",
        "print(f\"\\nScanned files: {scanned_files}\")\n",
        "print(f\"HF rows with unparsed y/v/p: {bad_rows}\")\n",
        "print(f\"Saved rerun checklist -> {todo_path}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
